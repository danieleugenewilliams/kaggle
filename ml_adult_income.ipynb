{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "This is my first attempt on Kaggle so I decided to try using the sklearn framework that I am most comfortable with right now. So far, I am able to get the accuracy to about 86.73%.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport csv\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"../input\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n# Any results you write to the current directory are saved as output.\"\n\n# set pandas chained_assignment flag = None here\npd.options.mode.chained_assignment = None",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def preprocess_target(dframe, df_column_name):\n    col = dframe[[df_column_name]]\n    le_col = LabelEncoder()\n    le_col.fit(np.ravel(col))\n    return le_col.transform(np.ravel(col))\n\ndef preprocess_features(dframe):\n    for column in dframe:\n        enc = LabelEncoder()\n        if(column not in ['age','education.num','fnlwgt','capital.gain','capital.loss','hours.per.week']):\n            dframe[column] = enc.fit_transform(dframe[column])\n    return dframe",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# import data and preprocess\ndf = pd.read_csv('../input/adult.csv')\n\n# select and preprocess features\nle_data = LabelEncoder()\nfeatures = ['age','workclass','education','marital.status','occupation','education.num','race','sex','relationship','capital.gain','capital.loss','native.country','income']\ndata = df[features]\ndata = preprocess_features(data)\n\n# select target\ntarget = data['income']\ndata = data.drop('income', axis=1)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# split train and test data\nX_train, X_test, y_train, y_test = train_test_split(\n    data, target, test_size=0.4, random_state=0)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# select algorithm\n#from sklearn.tree import DecisionTreeClassifier\n#from sklearn.ensemble import AdaBoostClassifier\n#clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),algorithm=\"SAMME\",n_estimators=200)\nfrom sklearn.ensemble import GradientBoostingClassifier\nclf = GradientBoostingClassifier(loss='deviance', n_estimators=100, learning_rate=1.0,max_depth=2, random_state=0)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# fit and predict\nclf.fit(X_train, y_train)\npredictions = clf.predict(X_test)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# display the relative importance of each attribute\nrelval = clf.feature_importances_\n\n# horizontal bar plot of feature importance\npos = np.arange(12) + 0.5\nplt.barh(pos, relval, align='center')\nplt.title(\"Feature Importance\")\nplt.xlabel(\"\")\nplt.ylabel(\"Features\")\nplt.yticks(pos, ('Age','Working Class','Education','Marital Status','Occupation','Education Grade','Race','Sex','Relationship Status','Capital Gain','Capital Loss','Native Country'))\nplt.grid(True)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# calc metrics\ntrue_negatives = 0\nfalse_negatives = 0\ntrue_positives = 0\nfalse_positives = 0\nfor prediction, truth in zip(predictions, y_test):\n    if prediction == 0 and truth == 0:\n        true_negatives += 1\n    elif prediction == 0 and truth == 1:\n        false_negatives += 1\n    elif prediction == 1 and truth == 0:\n        false_positives += 1\n    elif prediction == 1 and truth == 1:\n        true_positives += 1\n    else:\n        print (\"Warning: Found a predicted label not == 0 or 1.\")\n        print (\"All predictions should take value 0 or 1.\")\n        print (\"Evaluating performance for processed predictions:\")\n        break",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "try:\n    print(\"Test Dataset (40%):\")\n    print(\"true_positives\",true_positives)\n    print(\"true_negatives\",true_negatives)\n    print(\"false_positives\",false_positives)\n    print(\"false_negatives\",false_negatives)\n    total_predictions = true_negatives + false_negatives + false_positives + true_positives\n    print(\"total predictions:\",total_predictions)\n    accuracy = 1.0*(true_positives + true_negatives)/total_predictions\n    print(\"accuracy:\",accuracy)\n    precision = 1.0*true_positives/(true_positives+false_positives)\n    print(\"precision:\",precision)\n    recall = 1.0*true_positives/(true_positives+false_negatives)\n    print(\"recall\",recall)\n    f1 = 2.0 * true_positives/(2*true_positives + false_positives+false_negatives)\n    print(\"f1\",f1)\n    f2 = (1+2.0*2.0) * precision*recall/(4*precision + recall)\n    print(\"f2\",f2)\n    print (clf)\n    #print (PERF_FORMAT_STRING.format(accuracy, precision, recall, f1, f2, display_precision = 5))\n    #print (RESULTS_FORMAT_STRING.format(total_predictions, true_positives, false_positives, false_negatives, true_negatives))\n    print (\"\")\nexcept:\n    print (\"Got a divide by zero when trying out:\", clf)\n    print (\"Precision or recall may be undefined due to a lack of true positive predicitons.\")",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    }
  ]
}
